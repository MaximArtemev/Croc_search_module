{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import os\n",
    "import pymorphy2\n",
    "import re\n",
    "from stop_words import get_stop_words\n",
    "import pickle\n",
    "\n",
    "AllowedWords = '[а-я]+'\n",
    "\n",
    "wordNormal = pymorphy2.MorphAnalyzer()\n",
    "stop_words = {wordNormal.normal_forms(i)[0] for i in get_stop_words('ru')}\n",
    "\n",
    "\n",
    "MODEL_NAME = 'finaleModel.bin'\n",
    "\n",
    "#This is the part where all docs and discussions get cleaned and stored\n",
    "#Also test and train txt was created here\n",
    "\n",
    "fileMap = dict()\n",
    "\n",
    "for num,name in enumerate(os.listdir(\"./Dirty/Data/\")):\n",
    "    fileMap[num] = name\n",
    "    with open(\"./Dirty/Data/\"+name, 'r') as doc:\n",
    "        soup = BeautifulSoup(doc.read(), 'html.parser')\n",
    "        with open(\"./Clean/Data/\"+str(num)+'.txt', 'w') as clean:\n",
    "            clean.write(' '.join(soup.getText(' ').strip().split()))\n",
    "            \n",
    "text = ''\n",
    "for name in os.listdir(\"./Clean/Data/\"):\n",
    "    try:\n",
    "        with open(\"./Clean/Data/\"+name, 'r',) as doc:\n",
    "            for word in re.findall(AllowedWords, doc.read().lower()):\n",
    "                normal = wordNormal.normal_forms(word)[0]\n",
    "                text += str(normal) + ' '\n",
    "            text += '\\n'\n",
    "    except:\n",
    "        print(\"haha\")\n",
    "with open('./Technical/test.txt', 'w') as file:\n",
    "        file.write(text)\n",
    "        \n",
    "for num, name in enumerate(os.listdir(\"./Dirty/Discussions/\")):\n",
    "    with open(\"./Dirty/Discussions/\"+name, 'r') as doc:\n",
    "        soup = BeautifulSoup(doc.read(), 'html.parser')\n",
    "        with open(\"./Clean/Discussions/\"+str(num)+'.txt', 'w') as clean:\n",
    "            clean.write(' '.join(soup.getText(' ').strip().split()))\n",
    "            \n",
    "text = ''\n",
    "for name in os.listdir(\"./Clean/Discussions/\"):\n",
    "    try:\n",
    "        with open(\"./Clean/Discussions/\"+name, 'r') as doc:\n",
    "            for word in re.findall(AllowedWords, doc.read().lower()):\n",
    "                normal = wordNormal.normal_forms(word)[0]\n",
    "                text += str(normal) + ' '\n",
    "            text += '\\n'\n",
    "    except:\n",
    "        print(\"haha\")\n",
    "with open('./Technical/train.txt', 'w') as file:\n",
    "    file.write(text)\n",
    "\n",
    "\n",
    "#This is part where model was created\n",
    "    \n",
    "command = \"\"\"./sent2vec/fasttext sent2vec -input \\\n",
    "    ./Technical/train.txt -output ./Model/{} -minCount 2 -dim 700\\\n",
    "        -epoch 9 -lr 0.15 -wordNgrams 2 -loss ns -neg 10 -thread 2\\\n",
    "            -t 0.000005 -dropoutK 4 -minCountLabel 10\"\"\".format(MODEL_NAME)\n",
    "os.system(command)\n",
    "\n",
    "#Here we create vectors for each doc\n",
    "command = \"\"\"./sent2vec/fasttext print-sentence-vectors ./Model/{} < ./Technical/test.txt\"\"\".format(MODEL_NAME)\n",
    "output = os.popen(command).readlines()\n",
    "\n",
    "#Cleaned all docs\n",
    "clearVec = dict()\n",
    "with open('./Technical/test.txt', 'r') as test:\n",
    "    text = test.readlines()\n",
    "    for num, i in enumerate(output):\n",
    "        clearVec[(text[num].strip())] = [float(j) for j in i.strip().split()]\n",
    "\n",
    "#Let's store our clearVec with pickle\n",
    "\n",
    "with open('./Technical/clearVec.pickle', 'wb') as file:\n",
    "    pickle.dump(clearVec, file)\n",
    "    \n",
    "#And dont forgot our file mapping file\n",
    "\n",
    "with open('./Technical/fileMapping.pickle', 'wb') as file:\n",
    "    pickle.dump(fileMap, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'DOC-10091.html',\n",
       " 1: 'DOC-10092.html',\n",
       " 2: 'DOC-10093.html',\n",
       " 3: 'DOC-10094.html',\n",
       " 4: 'DOC-10392.html',\n",
       " 5: 'DOC-10422.html',\n",
       " 6: 'DOC-105943.html',\n",
       " 7: 'DOC-106600.html',\n",
       " 8: 'DOC-12204.html',\n",
       " 9: 'DOC-12223.html',\n",
       " 10: 'DOC-1464.html',\n",
       " 11: 'DOC-1473.html',\n",
       " 12: 'DOC-15172.html',\n",
       " 13: 'DOC-1554.html',\n",
       " 14: 'DOC-1555.html',\n",
       " 15: 'DOC-1556.html',\n",
       " 16: 'DOC-1571.html',\n",
       " 17: 'DOC-1572.html',\n",
       " 18: 'DOC-1578.html',\n",
       " 19: 'DOC-15821.html',\n",
       " 20: 'DOC-17395.html',\n",
       " 21: 'DOC-17643.html',\n",
       " 22: 'DOC-18044.html',\n",
       " 23: 'DOC-18078.html',\n",
       " 24: 'DOC-1902.html',\n",
       " 25: 'DOC-1936.html',\n",
       " 26: 'DOC-20926.html',\n",
       " 27: 'DOC-21430.html',\n",
       " 28: 'DOC-21638.html',\n",
       " 29: 'DOC-21641.html',\n",
       " 30: 'DOC-22163.html',\n",
       " 31: 'DOC-23526.html',\n",
       " 32: 'DOC-27881.html',\n",
       " 33: 'DOC-2837.html',\n",
       " 34: 'DOC-33678.html',\n",
       " 35: 'DOC-33688.html',\n",
       " 36: 'DOC-33999.html',\n",
       " 37: 'DOC-34224.html',\n",
       " 38: 'DOC-3660.html',\n",
       " 39: 'DOC-36664.html',\n",
       " 40: 'DOC-36815.html',\n",
       " 41: 'DOC-38728.html',\n",
       " 42: 'DOC-39159.html',\n",
       " 43: 'DOC-39297.html',\n",
       " 44: 'DOC-41855.html',\n",
       " 45: 'DOC-42104.html',\n",
       " 46: 'DOC-42205.html',\n",
       " 47: 'DOC-43368.html',\n",
       " 48: 'DOC-45372.html',\n",
       " 49: 'DOC-46243.html',\n",
       " 50: 'DOC-46673.html',\n",
       " 51: 'DOC-46810.html',\n",
       " 52: 'DOC-47223.html',\n",
       " 53: 'DOC-47280.html',\n",
       " 54: 'DOC-47282.html',\n",
       " 55: 'DOC-47718.html',\n",
       " 56: 'DOC-4837.html',\n",
       " 57: 'DOC-49441.html',\n",
       " 58: 'DOC-54202.html',\n",
       " 59: 'DOC-56913.html',\n",
       " 60: 'DOC-58852.html',\n",
       " 61: 'DOC-5924.html',\n",
       " 62: 'DOC-6040.html',\n",
       " 63: 'DOC-75951.html',\n",
       " 64: 'DOC-77819.html',\n",
       " 65: 'DOC-81439.html',\n",
       " 66: 'DOC-8839.html',\n",
       " 67: 'DOC-89980.html',\n",
       " 68: 'DOC-90817.html',\n",
       " 69: 'DOC-9564.html',\n",
       " 70: 'DOC-9566.html',\n",
       " 71: 'DOC-97164.html',\n",
       " 72: 'DOC-98644.html',\n",
       " 73: 'DOC-98653.html',\n",
       " 74: 'DOC-98682.html',\n",
       " 75: 'DOC-98740.html',\n",
       " 76: 'DOC-98913.html'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
